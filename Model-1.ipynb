{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx7Iy3p5OykV"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_0HpkmA6OmTY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTg7Yw7eSMaD"
      },
      "source": [
        "## Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Nbf0UP-MQggy"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W6GQA5rIT6p7"
      },
      "outputs": [],
      "source": [
        "csv_file = path + \"/IMDB Dataset.csv\"\n",
        "df = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FF7FkUIU9_S"
      },
      "source": [
        "## Text cleaning and Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TuRrdKr_YTu-"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "# Convert sentiments to binary values\n",
        "df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
        "\n",
        "# Clean text data\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<br />', ' ', text)  # Remove HTML tags\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
        "    return text\n",
        "\n",
        "df['cleaned_review'] = df['review'].apply(clean_text)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['cleaned_review'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df['cleaned_review'])\n",
        "max_length = 200\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Split into training (80%), dev (10%), and test (10%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(padded_sequences, df['sentiment'], test_size=0.2, random_state=42)\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK2piUg_bgt7"
      },
      "source": [
        "## Building and Training the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yjDwsnhblPU",
        "outputId": "6bef2b5a-6036-40d4-866b-c2bf2212774d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 268ms/step - accuracy: 0.5467 - loss: 0.6854 - val_accuracy: 0.7574 - val_loss: 0.5858\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 271ms/step - accuracy: 0.6434 - loss: 0.6127 - val_accuracy: 0.5914 - val_loss: 0.6428\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 274ms/step - accuracy: 0.7886 - loss: 0.4456 - val_accuracy: 0.8798 - val_loss: 0.2857\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 281ms/step - accuracy: 0.9118 - loss: 0.2277 - val_accuracy: 0.8862 - val_loss: 0.2756\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 275ms/step - accuracy: 0.9424 - loss: 0.1643 - val_accuracy: 0.8860 - val_loss: 0.2976\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_length))\n",
        "model.add(LSTM(units=64, return_sequences=False))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_dev, y_dev))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS5Wvl_tit10"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srxrcqHJiwQ_",
        "outputId": "fc6863e1-ea29-430a-a565-7cc2f63b5265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step\n",
            "F1-score: 0.8857949959644875\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1-score: {f1}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
